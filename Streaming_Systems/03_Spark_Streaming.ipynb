{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "- Spark Streaming implementa la aproximación micro-batch para el procesamiento de datos masivos en tiempo real.\n",
    "- Extensión del núcleo de la API Spark\n",
    "    - Se utiliza la sintaxis de Spark\n",
    "- Flujo de datos clásico\n",
    "    - Input: Kafka, Flume, Twitter...\n",
    "    - Output: HDFS, BBDD, dashboards...\n",
    "- El stream de entrada se divide en micro-batches y se procesa\n",
    "\n",
    "## Esquema básico\n",
    "- Flujo de entrada -> Micro-batches(tamaño \"delta\", medido en segundos)\n",
    "- Micro-batches -> Spark RDDs (denominado DStream)\n",
    "- Cada RDD se procesa con Spark (grafo de computación)\n",
    "    - Mismas transformaciones que en Spark (map, reduce, filter, join...)\n",
    "    - Funcionalidad adicional de Dstream\n",
    "- El resultado son micro-batches procesados\n",
    "- Spark Streming RDD = Spark RDD\n",
    "![03_spark_streaming_1]\n",
    "\n",
    "## Características principales\n",
    "- Capa de abstracción de alto nivel+\n",
    "    - Detalles de la implementación de streaming más transparentes al usuario\n",
    "- Mismo código que Spark clásico (reutilización de cualquier librería del ecosistema Spark)\n",
    "    - SparkSQL, SparkML, etc\n",
    "- Aproximación Micro-batch\n",
    "    - Alto Rendimiento\n",
    "    - Aumento de latencia (mayor retraso en el procesamiento ~ segundos)\n",
    "- Uso en arquitecturas Lambda:\n",
    "    - Misma arquitectura (Spark para offline, Spark Streaming para tiempo real)\n",
    "    - Se reduce la utilización de distintas tecnologías\n",
    "- No la mejor solución para arquitecturas Kappa\n",
    "    - No proporciona métodos sencillos para re-calcular datos históricos\n",
    "    \n",
    "## Aplicaciones Spark Streaming\n",
    "- Creación de Spark Streaming Context -> A partir de Spark Context -> Se especifica la duración del micro-batch\n",
    "\n",
    "[03_spark_streaming_1]:images/03_spark_streaming_1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, batchDuration = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entrada de datos -> Construcción de Dstream\n",
    "    - Entrada básica:\n",
    "        - fileStream(): carpeta con archivos (nuevos datos = nuevos ficheros añadidos)\n",
    "        - socketTextStream(): el input proviene del socket de una red (nuevos datos = datos recibidos en el socket durante el último intervalo de tiempo con duración \"batchDuration\")\n",
    "        - queueStream(): secuencia de RDDs de Spark, cada RDD se trata como un batch simple\n",
    "    - Entrada avazanzada:\n",
    "        - Kafka, Flume, Kinesis, ...\n",
    "        - Normalmente mediante el uso de librerías adicionales\n",
    "    - Entrada personalizada:\n",
    "        - Conectores ad-hoc\n",
    "\n",
    "- Ejemplo: Input con sockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, batchDuration = 10)\n",
    "\n",
    "dstream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la consola de comandos:\n",
    "\n",
    "- Si no existe el comando \"nc\" -> sudo apt-get install netcat\n",
    "\n",
    "> nc -lk 9999 (mantiene abierto un socket en el puerto 9999 de localhost)\n",
    "\n",
    "Se introduce el texto deseado (también se pueden utilizar pipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ejemplo: Input con Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, batchDuration = 10)\n",
    "\n",
    "dstream = KafkaUtils.createDirectStream(\n",
    "    scc,\n",
    "    topics = ['my_topic']\n",
    "    kafkaParams = {'metadata.broker.list':\n",
    "                      'kafka.br01:9092',\n",
    "                      'kafka.br02:9092',\n",
    "                      'kafka.br03:9092'}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones\n",
    "|Método|Funcionamiento|\n",
    "|---|---|\n",
    "|map(func)|Se aplica la función func a cada elemento de DStream. Devuelve un nuevo DStream|\n",
    "|flatMap(func)| Similar a map, pero por cada elemento de entrada se devuelven de 0 a n elementos de salida|\n",
    "|filter(func)|Nuevo DStream con los elementos del DStream original para los que func devuelva True|\n",
    "|repartition(nPartitions)|Modifica el nivel de paralelismo de un DStream|\n",
    "|union(otherStream)| Nuevo DStream con la unión del actual y de otherStream|\n",
    "|count()| Nuevo DStream con RDDs simples con el número de elementos de cada RDD en el DStream original|\n",
    "|reduce(func)| Agregación de elementos de cada RDD del DStream original para generar un nuevo DStream on RDDs simples. La función func ha de ser conmutativa y asociativa, recibir dos argumentos y evolver uno.|\n",
    "|countByValue()| Sobre un DStream de elementos de tipo K, devuelve la frecuencia de cada elemento del DStream en formato (K, Long)|\n",
    "|reduceByKey(func,[numTasks])|Sobre un DStream de elementos clave-valor (K,V), devuelve la agregación de todos los valores con la misma clave, según la función func. Se puede especificar el número de tareas paralelas de forma opcional (por defecto 2).|\n",
    "|join(otherStream,[numTasks])|Sobre dos DStreams (original y otherStream) de clave-valor (K,V) y (K,W), devuelve un DStream (K, (V,W)), sólo para aquéllas claves que aparezcan en ambos DStreams. Número de tareas paralelas opcional.|\n",
    "|cogroup(otherStream,[numTasks])|Sobre dos DStreams (original y otherStream) de clave-valor (K,V) y (K,W), devuelve un nuevo DStream con tuplas (K, Seq[V], Seq[W]), para todas las claves que aparezcan al menos en uno de los DStreams. Número de tareas paralelas opcional.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salida de datos\n",
    "    - dstream.pprint(): salida estándar (stout)\n",
    "    - dstream.saveAsTextFiles(): salida a almacenamiento externo (HDFS, AmazonS3, ...)\n",
    "    - dstream.foreachRDD(): salida manual personalizada (por ejemplo, a un almacenamiento clave-valor externo)\n",
    "        - Opción más utilizada\n",
    "        - La semántica ha de ser proporcionada por el desarrollador\n",
    "- Funciones para la ejecución del programa\n",
    "    - scc.start(): comienza los cálculos sin bloquear la ejecución del programa\n",
    "    - ssc.awaitTermination(): No permite la finalización prematura del porograma ( por ejemplo, si lo ejecutamos desde un sript Python)\n",
    "    \n",
    "- Ejecución desde la consola de comandos:\n",
    "    - Iniciar el flujo de datos (desde socker, kafka, etc.)\n",
    "    > spark-submit streaming_application.py\n",
    "    - Comenzar a enviar datos\n",
    "    - Finalización con Control-C\n",
    "    \n",
    "- Ejecución desde iPython o desde notebooks\n",
    "    - Iniciar el flujo de datos (desde socket, kafka, etc)\n",
    "    - Ejecutar el código (sin la función awaitTermination())\n",
    "    - Comenzar a enviar datos\n",
    "    - Ejecutar la función ssc.stop()\n",
    "    \n",
    "- Funcionalidades de Spark Streaming:\n",
    "    - dstream.window(...)\n",
    "        - Aproximación basada en Windowing\n",
    "        - Transformaciones sobre una ventana deslizante de los datos\n",
    "        - Características\n",
    "            - Tamaño (size)\n",
    "            - Desplazamiento (shift)\n",
    "            - El desplazamiento ha de ser múltiplo de la duración del batch, y el tamaño múltiplo del desplazamiento\n",
    "            - Ejemplo: batch=1 seg, shift = 2 seg, size = 5 seg\n",
    "\n",
    "Paso 1\n",
    "![03_spark_streaming_2]\n",
    "\n",
    "Paso 2\n",
    "![03_spark_streaming_3]\n",
    "\n",
    "Paso 3\n",
    "![03_spark_streaming_4]\n",
    "\n",
    "Paso 4\n",
    "![03_spark_streaming_5]\n",
    "\n",
    "- Transformaciones en ventanas:\n",
    "\n",
    "|Método| Funcionamiento|\n",
    "|---|---|\n",
    "|window(windowLength,slideInterval)|Nuevo DStream calculado según batches en ventanas del DStream original,tamaño de ventana windowLength y salto entre ventanas slideInterval|\n",
    "|countByWindow(windowLength, slideInterval)|Devuelve el conteo de elementos de la ventana deslizante definida con windowLength y slideInterval, sobre el DStream original|\n",
    "|reduceByWindow(func,windowLength,slideInterval)|Similar a reduce, permite la agregación de elementos de cada RDD del DStream original para generar un nuevo DStream con RDDs simples, sobre los batches de la ventana deslizante definida con windowLength y slideInterval. La función func ha de ser conmutativa y asociativa, recibir dos argumentos y devolver uno.|\n",
    "|educeByKeyAndWindow(func, windowLength,slideInterval, [numTasks])|Similar a reduceByKey, sobre un stream original con pares clave-valor (K,V),devuelve el agregado según la función func para cada clave, sobre los batches de la ventana deslizante definida con windowLength y slideInterval. Número de tareas paralelas opcional.|\n",
    "|countByValueAndWindow(windowLength,slideInterval, [numTasks])|Sobre un DStream de pares clave-valor (K,V), devuelve la frecuencia de cada clave en formato (K, Long), sobre los batches de la ventana deslizante definida con windowLength y slideInterval. Número de tareas paralelas opcional.|\n",
    "\n",
    "[03_spark_streaming_2]:images/03_spark_streaming_2.png\n",
    "[03_spark_streaming_3]:images/03_spark_streaming_3.png\n",
    "[03_spark_streaming_4]:images/03_spark_streaming_4.png\n",
    "[03_spark_streaming_5]:images/03_spark_streaming_5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformaciones stateful:\n",
    "    - dstream.updateStateByKey(upadate_func, ...)\n",
    "        - Almacenamiento de pares clave-valor y actualización con nuevos datos -> Cálculos stateful\n",
    "        - Se ha de definir un directorio para el checkpoint\n",
    "        - Se ha de especificar la función para actualizar los datos\n",
    "            - La función recibe los nuevos valores (de cada clave), así como el estado anterior (si existe)\n",
    "            - Devuelve un nuevo valor, del mismo tipo que el estado guardado\n",
    "\n",
    "- Transformaciones stateful (ejemplo): ocurrencias de palabras guardando el total de apariciones de cada una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Recibe la lista de nuevos valores para cada key, y último estado guardado\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).updateStateByKey(updateFunc)\n",
    "running_counts.pprint()\n",
    "\n",
    "context.start()\n",
    "context.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checkpointing\n",
    "    - Recuperación del estado del sistema después de un fallo -> procesamiento de datos desde el momento del fallo\n",
    "    - Se almacena información sobre el estado del sistema (metadatos o RDDs) en un almacenamiento tolerante a fallos (por ejemplo, HDFS)\n",
    "    - Backup para regresar al último punto estable\n",
    "    - Permite restaurar valores de operaciones internas stateful como updateStateByKey\n",
    "    - Creación periódica -> Definición manual del intervalo\n",
    "        - Cuanto más pequeño, se reduce el rendimiento\n",
    "        - Recomendado 5-10 batchDurations\n",
    "    - El uso de checkpoints no soporta actualizaciones de código\n",
    "        - Para elo habría que correr dos instancias de SparkStreaming (una antigua y una nueva) -> recursos limitados, pérdida de estados internos, etc.\n",
    "        - O almacenar manualmente los datos antiguos antes de inicializar la nueva versión -> Se pierden las ventajas del checkpointing\n",
    "    - Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "checkpoint_directory = '/hdfs://path/to/cp' # Definimos la ruta de almacenamiento\n",
    "\n",
    "def createContext(): # La lógica de la aplicación se incluye en una función\n",
    "    sc = SparkContext(...)\n",
    "    ssc = StreamingContext(...)\n",
    "    dstream = ... # Lógica de la aplicación\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    ssc.checkpoint(checkpoint_directory) # Se almacenan los datos\n",
    "    return ssc\n",
    "\n",
    "# La función getOrCreate genera el contexto si se trata de la primera ejecución, \n",
    "# o recupera los datos de checkpoint en caso contratrio\n",
    "context = StreamingContext.getOrCreate(checkpoint_directory, get_context)\n",
    "context.start()\n",
    "context.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semántica de entrega y tolerancia a fallos en Spark Streaming\n",
    "- ¿Qué semántica de entrega necesito asegurar?\n",
    "- ¿Cómo puedo proporcionarla?\n",
    "- Depende de todos los elementos del pipeline de streaming y de las interacciones entre ellos\n",
    "- Siempre se solicitará semántica exactly-once\n",
    "    - Muy difícil de proporcionar (y no siempre se necesita)\n",
    "    - Se necesita conseguir en la captura de datos, en su procesamiento y en su almacenamiento\n",
    "    - Spark Streaming -> exactly-once únicamente en sus cálculos internos durante la fase de procesamiento\n",
    "- Recepción de datos\n",
    "    - Semántica de entrega dependientes de las fuentes (desde at-least-once a exactly-once)\n",
    "- Transformación de datos\n",
    "    - La semántica de entrega interna en DStreams y RDDs es siempre exactly-once\n",
    "- Salida de datos\n",
    "    - Normalmente (por ejemplo, en la función foreachRDD) se asegura la salida at-least-once. La semántica exactly-once ha de ser implementada por los usuarios\n",
    "- Estrategias adicionales: Write-ahead Log (WAL)\n",
    "    - Permite realizar checkpointing sin pérdida de datos, aunque la fuente de datos de entrada no permita su re-envío (asegura at-leat-once para cualquier fuente de datos)\n",
    "    - Para ello, se almacenan todos los datos recibidos en un sistema de ficheros tolerante a fallos\n",
    "        - Sólo se considera leído el evento cuando se ha escrito en WAL\n",
    "        - La aplicación lee los datos de WAL en lugar de la fuente\n",
    "    - En SparkStreaming\n",
    "        - Uso de chekpoint directoy -> WAL se almacena también\n",
    "        - En los parámetros de configuración que se pueden definir al crear SparkContext hay que especificar \"spark.streaming.receiver.WriteAheadLog.enable = True\"\n",
    "        - Se evita la pérdida de datos\n",
    "        - Se aumenta el tiempo de procesamiento de cada batcha\n",
    "        > conf.set(\"spark.streaming.receiver.WriteAheadLog.enable\", \"True\")\n",
    "        >\n",
    "        > sc = SparkContext(conf)\n",
    "        >\n",
    "        > ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
