{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de instalación\n",
    "\n",
    "- Para la realización de pruebas lo recomendable es tener Spark en una máquina simple, donde además tendremos datasets de tamaño pequeño.\n",
    "\n",
    "- Para la conexión a clusters dedicados utilizaremos conexiones SSH\n",
    "    - Sin gestor de cluster externo: Standalone\n",
    "    - Con gestor\n",
    "        - Mesos\n",
    "        - YARN\n",
    "    - Clusters en la nube\n",
    "        - EC2\n",
    "        - Kubernetes\n",
    "    - Generación de Spark a partir del código fuente\n",
    "        - Maven/Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para la instalación en local, lo primero es ir a la página de Spark y seleccionar la versión.\n",
    "    - Es más sencillo bajarnos la versión con hadoop integrado.\n",
    "![spark_install_1]\n",
    "\n",
    "\n",
    "[spark_install_1]:images/spark_install_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo en Scala\n",
    "\n",
    "Extraemos el archivo y ejecutamos\n",
    "\n",
    "bin/spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 15.0.1)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = textFile.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words_init_1 = words.map(word => (word,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words_init_1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words_counted = words_init_1.reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counted: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counted.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res0: Array[(String, Int)] = Array((package,1), (this,1), (integration,1), (Python,2), (cluster.,1), (its,1), ([run,1), (There,1), (general,2), (have,1), (pre-built,1), (Because,1), (YARN,,1), (locally,2), (changed,1), (locally.,1), (several,1), (only,1), (Configuration,1), (This,2), (basic,1), (first,1), (learning,,1), (documentation,3), (graph,1), (Hive,2), (info,1), ([\"Specifying,1), (\"yarn\",1), ([params]`.,1), ([project,1), (prefer,1), (SparkPi,2), (engine,2), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (are,1), (systems.,1), (params,1), (scala>,1), (DataFrames,,1), (provides,1), (refer,2), (configure,1), (Interactive,2), (R,,1), (can,6), (build,3), (when,1), (easiest,1), (Maven](https://maven.apache.org/).,1), (Apache,1), (guide](https://spark.apache.org/con..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " words_counted.filter( words => words._2 > 10).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1: Array[(String, Int)] = Array((\"\",73), (Spark,14), (for,12), (the,23), (to,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo en Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el shell de pyspark\n",
    "\n",
    "bin\\pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = textFile.flatMap(lambda line:line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 20/11/05 16:20:46 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_init_1 = words.map(lambda word:(word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counted = words_init_1.reduceByKey(lambda a,b:a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counted.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[('#', 1), ('Apache', 1), ('Spark', 14), ('', 73), ('is', 7), ('unified', 1), ('analytics', 1), ('engine', 2), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 5), ('Scala,', 1), ('Java,', 1), ('an', 4), ('optimized', 1), ('supports', 2), ('computation', 1), ('analysis.', 1), ('set', 2), ('of', 5), ('tools', 1), ('SQL', 2), ('MLlib', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('processing,', 1), ('Structured', 1), ('<https://spark.apache.org/>', 1), ('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 1), ('Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)', 1), ('Documentation', 1), ('latest', 1), ('programming', 1), ('guide,', 1), ('[project', 1), ('page](https://spark.apache.org/documentation.html).', 1), ('README', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('Building', 1), ('using', 3), ('[Apache', 1), ('run:', 1), ('do', 2), ('this', 1), ('downloaded', 1), ('documentation', 3), ('project', 1), ('site,', 1), ('at', 2), ('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1), ('development', 1), ('tips,', 1), ('developing', 1), ('IDE,', 1), ('[\"Useful', 1), ('Developer', 1), ('Interactive', 2), ('Shell', 2), ('The', 1), ('way', 1), ('start', 1), ('Try', 1), ('following', 2), ('scala>', 1), ('spark.range(1000', 2), ('*', 4), ('1000).count()', 2), ('Python', 2), ('Alternatively,', 1), ('use', 3), ('And', 1), ('run', 7), ('Example', 1), ('several', 1), ('programs', 2), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('./bin/run-example', 2), ('SparkPi', 2), ('variable', 1), ('when', 1), ('examples', 2), ('spark://', 1), ('URL,', 1), ('YARN,', 1), ('\"local\"', 1), ('locally', 2), ('N', 1), ('abbreviated', 1), ('class', 2), ('name', 1), ('package.', 1), ('instance:', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('Testing', 1), ('Spark](#building-spark).', 1), ('Once', 1), ('built,', 1), ('tests', 2), ('using:', 1), ('./dev/run-tests', 1), ('Please', 4), ('guidance', 2), ('module,', 1), ('individual', 1), ('integration', 1), ('test,', 1), ('Note', 1), ('About', 1), ('uses', 1), ('library', 1), ('HDFS', 1), ('other', 1), ('Hadoop-supported', 1), ('storage', 1), ('systems.', 1), ('Because', 1), ('have', 1), ('changed', 1), ('different', 1), ('versions', 1), ('Hadoop,', 2), ('must', 1), ('against', 1), ('version', 1), ('refer', 2), ('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 1), ('particular', 2), ('distribution', 1), ('Hive', 2), ('Thriftserver', 1), ('distributions.', 1), ('[Configuration', 1), ('online', 1), ('overview', 1), ('configure', 1), ('Spark.', 1), ('Contributing', 1), ('guide](https://spark.apache.org/contributing.html)', 1), ('started', 1), ('contributing', 1), ('project.', 1), ('a', 9), ('for', 12), ('large-scale', 1), ('data', 2), ('processing.', 2), ('Python,', 2), ('and', 9), ('R,', 1), ('that', 2), ('general', 2), ('graphs', 1), ('also', 5), ('rich', 1), ('higher-level', 1), ('including', 4), ('DataFrames,', 1), ('Streaming', 1), ('stream', 1), ('[![Jenkins', 1), ('Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)', 1), ('[![AppVeyor', 1), ('[![PySpark', 1), ('##', 9), ('Online', 1), ('You', 3), ('can', 6), ('find', 1), ('the', 23), ('documentation,', 1), ('on', 7), ('web', 1), ('This', 2), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('Maven](https://maven.apache.org/).', 1), ('To', 2), ('build', 3), ('its', 1), ('example', 3), ('programs,', 1), ('./build/mvn', 1), ('-DskipTests', 1), ('clean', 1), ('package', 1), ('(You', 1), ('not', 1), ('need', 1), ('to', 16), ('if', 4), ('you', 4), ('pre-built', 1), ('package.)', 1), ('More', 1), ('detailed', 2), ('available', 1), ('from', 1), ('[\"Building', 1), ('For', 3), ('info', 1), ('see', 3), ('Tools\"](https://spark.apache.org/developer-tools.html).', 1), ('Scala', 2), ('easiest', 1), ('through', 1), ('shell:', 2), ('./bin/spark-shell', 1), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('1,000,000,000:', 2), ('1000', 2), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('Programs', 1), ('comes', 1), ('with', 3), ('sample', 1), ('`examples`', 2), ('directory.', 1), ('one', 2), ('<class>', 1), ('will', 1), ('Pi', 1), ('locally.', 1), ('MASTER', 1), ('environment', 1), ('running', 1), ('submit', 1), ('cluster.', 1), ('be', 2), ('mesos://', 1), ('or', 3), ('\"yarn\"', 1), ('thread,', 1), ('\"local[N]\"', 1), ('threads.', 1), ('MASTER=spark://host:7077', 1), ('Many', 1), ('given.', 1), ('Running', 1), ('Tests', 1), ('first', 1), ('requires', 1), ('[building', 1), ('how', 3), ('[run', 1), ('tests](https://spark.apache.org/developer-tools.html#individual-tests).', 1), ('There', 1), ('Kubernetes', 1), ('resource-managers/kubernetes/integration-tests/README.md', 1), ('A', 1), ('Hadoop', 3), ('Versions', 1), ('core', 1), ('talk', 1), ('protocols', 1), ('same', 1), ('your', 1), ('cluster', 1), ('runs.', 1), ('[\"Specifying', 1), ('Version', 1), ('Enabling', 1), ('building', 2), ('Configuration', 1), ('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1), ('review', 1), ('[Contribution', 1), ('information', 1), ('get', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counted.filter(lambda word:word[1]>10).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[('Spark', 14), ('', 73), ('for', 12), ('the', 23), ('to', 16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos se parecen mucho tanto pyspark como spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mas sobre los Ejemplos\n",
    "\n",
    "- En la web están en Python / Scala / Java\n",
    "- Los jobs se pueden seguir en http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Standalone\n",
    "\n",
    "En linux:\n",
    "\n",
    "- Scripts:\n",
    "    - Master: sbin/start-master.sh\n",
    "    - Workers: sbin/start-slave.sh\n",
    "\n",
    "En Windows:\n",
    "\n",
    "- Comandos:\n",
    "    - Master: %SPARK_HOME%\\bin\\spark-class org.apache.spark.deploy.master.Master (nos dará un mensaje estilo esto:\n",
    "\n",
    "20/11/05 16:32:44 INFO Master: Starting Spark master at spark://localhost:7077\n",
    "\n",
    "20/11/05 16:32:44 INFO Master: Running Spark version 3.0.1\n",
    "\n",
    "20/11/05 16:32:45 INFO Utils: Successfully started service 'MasterUI' on port 8080.\n",
    "\n",
    "20/11/05 16:32:45 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://XXXXXXXXX:8080\n",
    "\n",
    "20/11/05 16:32:45 INFO Master: I have been elected leader! New state: ALIVE\n",
    "    \n",
    "    - Workers: %SPARK_HOME%\\bin\\spark-class org.apache.spark.deploy.worker.Worker spark://XXXXXXXXX:port(por defecto 7077)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo ejecutar un ejemplo en el Cluster\n",
    "\n",
    "- run-example --master spark://XXXXXXXXX:7077 GroupByTest\n",
    "- spark-submit --master spark://XXXXXXXXX:7077 examples\\src\\main\\python\\pi.py\n",
    "\n",
    "Para conectar con el cluster en una sesión interactiva:\n",
    "\n",
    "- spark-shell --master spark://XXXXXXXXX:7077\n",
    "- pypspark --master spark://XXXXXXXXX:7077"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
