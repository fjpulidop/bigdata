{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "- Facilidad de uso: en map reduce se simplifica mucho el desarrollo\n",
    "- Framework distribuido y altamente escalable para la computación paralela (en memoria) de gran cantidad de datos.\n",
    "- Soporte para varios lenguajes: Scala, Java, Python y R.\n",
    "- Permite trata la información en tiempo real utilizando técnicas de Streaming.\n",
    "\n",
    "## Componentes\n",
    "\n",
    "- RDD: Resilient Distributed Datasets, son estructuras de datos distribuidos que se pueden asignar a distintas instancias computacionales. Se le pueden aplicar transformaciones (manipulación) o acciones(hacer efectivo los cambios en los RDDs).\n",
    "- Cluster Manager: tienen que ser capaces de conocer los recursos computacionales que tenemos entre manos y ser capaces de distribuir el código y los datos en los distintos nodos. YARN, Mesos, Kubernetes ...\n",
    "\n",
    "## Uso en Ciencia de Datos\n",
    "\n",
    "- Capacidad de definir de forma semántica el contenido de las columnas de un Datasets. I.e. saber si son cadenas de texto ...\n",
    "- Soporte para R a través de SparkR\n",
    "\n",
    "## Plataforma de APIs\n",
    "\n",
    "- Se posee un API para integrar datos de fuentes externas.\n",
    "- Desarrollo de los paquetes de Spark para ir incluyendo funcionalidades.\n",
    "- Python se soporta con un API sobre Scala.\n",
    "- Se trabaja en facilitar el despliegue en Cloud (AWS, GCP, etc)\n",
    "\n",
    "## Optimización de Ejecución (Proyecto Tungsten)\n",
    "\n",
    "- Mejora en el rendimiento de las ejecuciones\n",
    "\n",
    "## Streaming, DAG, Visualización & Debug\n",
    "\n",
    "- Control del flujo con Spark Streaming\n",
    "\n",
    "## Full Stack \n",
    "\n",
    "![spark_1]\n",
    "\n",
    "\n",
    "[spark_1]: images/spark_1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones en Spark\n",
    "\n",
    "- Las aplicaciones Spark consistenen un proceso controlador (driver) y un conjunto de procesos ejecutores (executors).\n",
    "    - El proceso controlador (driver) se ejecuta en un nodo del clúster (normalmente el máster) y es responsable de tres cosas:\n",
    "        1. Mantener la información sobre la aplicación Spark.\n",
    "        2. Responder al programa de un usuario.\n",
    "        3. Analizar, distribuir y programar el trabajo entre los ejecutores.\n",
    "        ![spark_2]\n",
    "\n",
    "- Los ejecutores son responsables de:\n",
    "    - Ejecutar el código que se le ha asignado\n",
    "    - Informar del estado del cáculo\n",
    "\n",
    "- Spark soporta dos modos\n",
    "    - local[N]: se ejecuta en nuestra máquina, se le puede indicar si es batch y un número N de tareas.\n",
    "    - Cluster\n",
    "        - Gestiona las máquinas/contenedores: el modo en contenedores no es recomendable en producción.\n",
    "        - Reservas de recursos para las aplicaciones Spark\n",
    "        ![spark_3]\n",
    "\n",
    "[spark_2]:images/spark_2.png\n",
    "[spark_3]:images/spark_2.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Managers\n",
    "    \n",
    "- Standalone\n",
    "- Apache Mesos\n",
    "- Hadoop YARN\n",
    "- EC2(cloud): hay scripts para desplegar en la nube automáticamente\n",
    "- Kubernetes\n",
    "![spark_4]\n",
    "\n",
    "El driver es un contexto de la aplicación Spark, que se comunicará con el Cluster Manager que se ocupará de conocer el estado de los nodos trabajadores y hacer asignación de recursos computacionales. Una vez asignado los n nodos disponibles, el driver conectará con los nodos y mandará las ejecuciones y recibirá el resultado.\n",
    "\n",
    "[spark_4]:images/spark_4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenguajes de APIs en Spark\n",
    "\n",
    "- Scala/Java\n",
    "    - Son nativas y se ejecutan en la JVM\n",
    "- SQL: con SparkSQL, estilo Hive en Hadoop\n",
    "- Python/R: en python pySpark\n",
    "\n",
    "- Objeto SparkSession\n",
    "    - SparkSession será la representación de la aplicación (trabajo) para ejecutar el código Spark.\n",
    "    - Python/R -> el usuario nunca escribe instrucciones explícitas de JVM\n",
    "        - El código se traduce de manera transparente que Spark puede ejecutar en las JVMs de los ejecutores. Aunque escribamos en código python realmente se ejecuta en Scala en la JVM, por lo que será más lento en python. Si el rendimiento es importante es mejor ejecutarlo directamente en Scala.\n",
    "![spark_5]\n",
    "\n",
    "[spark_5]:images/spark_5.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
