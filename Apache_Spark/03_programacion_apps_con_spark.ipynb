{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entornos de desarrollo de Spark\n",
    "\n",
    "Tenemos estas opciones para trabajar con Spark\n",
    "\n",
    "- Scala (Spark-Shell)\n",
    "- Python (Pyspark)\n",
    "    - Pertime trabajar con Jupyter Notebooks\n",
    "- R (SparkR)\n",
    "\n",
    "Entornos de compilación de trabajos: aplicaciones que no necesiten tanta interactividad\n",
    "\n",
    "- Scala/Java (Eclipse / Intellij)\n",
    "- Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos utlilizar PySpark con Jupyter, podemos ejectuar estos comandos:\n",
    "\n",
    "- Linux:\n",
    "    \n",
    "    export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "    \n",
    "    export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "    \n",
    "- Windows:\n",
    "\n",
    "    set PYSPARK_DRIVER_PYTHON=jupyter\n",
    "    \n",
    "    set PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "    \n",
    "<SPARK_HOME>\\bin\\pyspark\n",
    " \n",
    "- Es conveniente ejecutarlo desde el directorio donde estén ubicados los notebooks/datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession y SparkContext\n",
    "\n",
    "- Antes de Spark 2.x: SparkContext, SqlContext y HiveContext\n",
    "    - SparkContext representaba la conexión con el entorno de ejcución de Spark\n",
    "    \n",
    "- Spark 2.x introduce el API de alto nivel para Datasets / Dataframes\n",
    "    - SparkSession pasa a ser la nueva representación de la conexión\n",
    "        - org.apache.spark.sql.SparkSession\n",
    "        - pyspark.sql.sparkSession\n",
    "    - El objeto SparkSession encapsula ahora a SparkContext como una propiedad\n",
    "\n",
    "- Para crear/leer/escribir Datasets/Dataframes => SparkSession\n",
    "\n",
    "- Para crear/leer/escribir RDD => SparkContext\n",
    "\n",
    "- Spark-shell y pyspark crean los objetos automáticamente\n",
    "    \n",
    "    val sparkSession = new\n",
    "    \n",
    "    SparkSession.builder.master(master_path).appName(\"application name\").config(\"optional configuration parameters\".getOrCreate()\n",
    "        - getOrCreate(): devuelve una sesión si existe o crea sino existe.\n",
    "    \n",
    "- Propiedades/Métodos importantes del objeto\n",
    "    - appName\n",
    "    - getConf\n",
    "    - getExecutorMemoryStatus\n",
    "    - Master\n",
    "    - Version\n",
    "    \n",
    "En el caso de escala todos los métodos que utilicemos son objetos. Normalmente el resultado de las opreaciones en scala son RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comparten APIs en las versiones Scala y Python, pero hay algunas funciones/propiedades que no están en Python (getExecutorMemoryStatus)\n",
    "\n",
    "|   Método\t|   Uso\t|\n",
    "|---\t|---\t|\n",
    "|   addJar(path)\t|   Añade archvios JAR para todos los jobs futuros que puedan ser ejecutados bajo SparkContext\t|\n",
    "|   addFile(path)\t|   Este método descarga un archivo de todos los nodos en el cluster\t|\n",
    "|   listFiles/listJars\t|   Muestra los archivos de todos los archivos/JARs añadido actualmente\t|\n",
    "|   stop()\t|   Para el SparkContext\t|\n",
    "|   clearFiles()\t|   Borra los archivos así que los nodos nuevos no lo descargarán\t|\n",
    "|   clearJars()\t|   elimina los JARs que se requieran en los jobs futuros\t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos necesarios para construir un programa en Spark (no en jupyter)\n",
    "\n",
    "- En Jupyter crea por defecto el SparkContext(sc) y el SparkSession(spark).\n",
    "\n",
    "- La aplicación debe crearlos explícitamente dentro del código\n",
    "    - Añadir\n",
    "        \n",
    "        ``from pyspark.sql import SparkSession`` \n",
    "    \n",
    "    - Crear la sesión Spark y obtener el contexto spark\n",
    "    \n",
    "        ``spark = SparkSession.builder.appName(\"Pi estimation, python job\").getOrCreate()``\n",
    "        \n",
    "        ``sc = spark.sparkContext``\n",
    "    \n",
    "    - Cerrar la sesión al final\n",
    "        \n",
    "        ``spark.stop()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación un ejemplo de como quedaría un archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x,y = random.random(),random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Pi estimation, python Job\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sc.parallelize(range(0,NUM_SAMPLES)).filter(inside).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142300\n"
     ]
    }
   ],
   "source": [
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
