{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Una de las aplicaciones más interesantes que se pueden realizar sobre la información que se recopila de Twitter es el denominado análisis de sentimiento o sentiment analysis. Se trata fundamentalmente de clasificar de forma automática la polaridad de un tweet en concreto, en fucnión de su contenido, categorizándolo como positivo, negativo o neutro, e incluso asignándole un valor numérico a dicha clasificación. Esta funcionalidad se utiliza, por ejemplo, en sistemas de análisis de reputación online (Online Reputation Managment, ORM) para determinar qué se está diciendo de una determinada marca en Twitter, en tiempo real.\n",
    "\n",
    "En este ejercicio vamos a simular, de forma simplificada, un sistema de tiempo real que ofrezca una puntuación de polaridad (positiva, negativa o neutra) a los tweets que vayan llegando. Para ello, el modelo de clasificación se basará en listas de palabras, denominadas lexicones. En concreto, dispondremos de un lexicón d palabras positivas y su puntuación, \"positive_lex.txt\", y de un lexicón de palabras negativas y su puntuación, \"negative_lex.txt\". Ambos contienen, en cada línea, una palabra (positiva o negativa, respectivamente), seguida de su puntuación. Nótese que la polaridad negativa ya está expresada con un signo menos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide desarrollar un notebook de Jupyter, denominado \"sentiment.ipynb\", en el que se utilice como fuente de datos Kafka, y en concreto el topic kafkaTwitter.\n",
    "\n",
    "Para cada nuevo tweet que llegue al sistema, se analizará su polaridad dividiendo el tweet por palabras y realizando la suma de la polaridad de todas las laplabras que aparezcan en los ficheros \"positive_lex.txt\" y \"negative_lex.txt\". Las palabras de lexicon negativo tienen un signo menos delante, por lo que restará a la puntuación final del tweet. Si ninguna de las palabras del tweet está almacenada en los ficheros proporcionados (o si las puntuaciones de las palabras positivas y negativas se anulan), la polaridad del tweet será 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida del sistema ha de imprimirse cada segundo, y consistirá en una línea por cada tweet recibido, que contenga el texto del propio tweet, a continuación la puntuación de su polaridad, y la palabra \"NEUTRO\" si la puntuación es igual a 0, \"POSITIVO\" si es mayor que 0, y \"NEGATIVO\" si es menor que 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, tenemos el siguiente tweet:\n",
    "\n",
    "\"Hoy vuelvo a España, me ha encantado Polonia :D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sistema devolverá el texto del tweet, seguido de la puntuación 0.417, y de la palabra \"POSITIVO\", ya que la palabra \"encantado\" está en el fichero \"positive_lex.txt\" con esa puntuación, y el resto de las palabras no se encuentran en ninguno de los dos lexicones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recomienda modificar el ritmo de volcado de tweets a Kafka, para que la salida sea más legible. Para ello, modificar la línea 24 del script \"tweet_producer.py\" de sleep(random.random()/10) a sleep(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import requests\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubicación de spark\n",
    "findspark.init('/home/j/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesario para cargar la librería de spark streaming y kafka\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre del topic\n",
    "topic = 'kafkaTwitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de los contextos de spark\n",
    "sc = SparkContext(\"local[*]\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación del DStream a partir de Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'localhost:9092, localhost:9093',\n",
    "                        'group.id':'exercise3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ventana de recepción de tweets de 20 segundos\n",
    "lines = kafkaStream.window( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estrucutra de la tupla de los tweets\n",
    "fields = (\"content\")\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención de los tweets\n",
    "( lines.map( lambda rec: Tweet( rec[1] ) ) # Almacenar los tweets en un objeto\n",
    "  .foreachRDD( lambda rdd: rdd.toDF() # transformarlos en dataframe\n",
    "  .limit(1).registerTempTable(\"tweets\") ) # registrarlos en una tabla\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estructura de la tupla de los lexicones\n",
    "fields = (\"word\",\"score\")\n",
    "Lexicon = namedtuple( 'Lexicon', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtención del lexicón positivo y su almacenado en una tabla\n",
    "positive_lex_rdd = sc.textFile(\"/home/j/spark-2.4.7-bin-hadoop2.7/python/TP3/resources/positive_lex.txt\")\n",
    "positive_lex_rdd_pl = ( positive_lex_rdd\n",
    "  .map ( lambda rec: Lexicon(rec.split(\" \")[0], rec.split(\" \")[1]))\n",
    ")\n",
    "positive_lex_df = sqlContext.createDataFrame(positive_lex_rdd_pl)\n",
    "positive_lex_df.registerTempTable('positive_lex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtención del lexicón negativo y su almacenado en una tabla\n",
    "negative_lex_rdd = sc.textFile(\"/home/j/spark-2.4.7-bin-hadoop2.7/python/TP3/resources/negative_lex.txt\")\n",
    "negative_lex_rdd_pl = ( negative_lex_rdd\n",
    "  .map ( lambda rec: Lexicon(rec.split(\" \")[0], rec.split(\" \")[1]))\n",
    ")\n",
    "negative_lex_df = sqlContext.createDataFrame(negative_lex_rdd_pl)\n",
    "negative_lex_df.registerTempTable('negative_lex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicio del stream\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:   'The Big Bang Theory' crea un programa de becas para ayudar a universitarios científicos http://t.co/TnJzDoSbEo http://t.co/9aC103yQjt\n",
      "Tweet score:  -0.09999999999999998\n",
      "Tweet sentiment: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# mientras sea verdad continuar con el stream\n",
    "while True:\n",
    "    # le pongo un temporizador de 2 segundos por tweet\n",
    "    time.sleep( 2 )\n",
    "    \n",
    "    # cargo el tweet\n",
    "    tweet = sqlContext.sql( '''\n",
    "        select\n",
    "            content\n",
    "        from tweets\n",
    "    ''' )\n",
    "    \n",
    "    # creo un dataframe del tweet\n",
    "    tweet_df = tweet.toPandas()\n",
    "    \n",
    "    # imprimo el tweet\n",
    "    print(\"Tweet: \",tweet_df['content'].to_string(index=False))\n",
    "    \n",
    "    # creo lista para almacenar las puntuaciones del tweet\n",
    "    total_score = []\n",
    "    \n",
    "    # recorro las palabras del tweet\n",
    "    for word_list in tweet_df['content'].str.split():\n",
    "        for word in word_list:\n",
    "            \n",
    "            # para cada palabra busco si se encuentra en los lexicones: \n",
    "            # - esto lo podía haber hecho de múltiples maneras, incluso más eficientes usando diccionarios etc, pero \n",
    "            #   quería utilizar spark sql para recorrer los lexicones\n",
    "            word_score1 = sqlContext.sql( '''\n",
    "                select\n",
    "                    sum(score) as word_score\n",
    "                from\n",
    "                    (\n",
    "                    select\n",
    "                        score\n",
    "                    from positive_lex\n",
    "                    where word = \"'''+word.replace('\"','').lower()+'''\"\n",
    "                    union\n",
    "                    select\n",
    "                        score\n",
    "                    from negative_lex\n",
    "                    where word = \"'''+word.replace('\"','').lower()+'''\"\n",
    "                    )\n",
    "            ''' )\n",
    "            \n",
    "            # almacenamos las puntuaciones en la lista \n",
    "            total_score.append(word_score1.first())\n",
    "\n",
    "        # creamos un dataframe con las puntuaciones\n",
    "        total_score_df = pd.DataFrame(total_score,columns=['word_score'])\n",
    "        \n",
    "        # hacemos la suma de las puntuaciones\n",
    "        tweet_score = total_score_df['word_score'].sum()\n",
    "        \n",
    "        # imprimios la puntuación\n",
    "        print(\"Tweet score: \", tweet_score)\n",
    "        \n",
    "        # por último imprimos el sentimiento en función de la puntuación\n",
    "        if(tweet_score==0):\n",
    "            print(\"Tweet sentiment: NEUTRAL\")\n",
    "        if(tweet_score<0):\n",
    "            print(\"Tweet sentiment: NEGATIVE\")\n",
    "        if(tweet_score>0):\n",
    "            print(\"Tweet sentiment: POSITIVE\")\n",
    "\n",
    "    # borramos el log de los tweets procesados anteriormente\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # nota: dejo la salida de un tweet y su resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
