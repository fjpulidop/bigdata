{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE Y PIG\n",
    "\n",
    "## En alto nivel... ¿Qué es HIVE y PIG?\n",
    "- Herramientas de programación\n",
    "    - HIVE: nos permite lanzar consultas tipo SQL contra archivos de HDFS\n",
    "    - PIG: es un nivel de abstracción superior a MapReduce que nos permite ejecutar comandos para el tratamiento avanzado de datos\n",
    "\n",
    "## Diferencias\n",
    "\n",
    "|Pig| Hive|\n",
    "|---|---|\n",
    "|Lenguaje procedural.| Lenguaje declarativo similar a SQL.|\n",
    "|Datos des/estructurados|Centrado en datos estructurados.|\n",
    "|Esquema definido en el programa.|Esquema definido de antemano.|\n",
    "|Principalmente usado para programar.| Principalmente usado para generar informes.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE\n",
    "\n",
    "## En detalle ... ¿Qué es HIVE?\n",
    "- Hive es una infraestructura de data warehouse sobre Hadoop que permite la utilización de un cluster de Hadoop sin necesidad de programar MapReduce\n",
    "- Utiliza un lenguaje de consultas similar a SQL, llamado HiveQL, de forma que desarrolladores que conozcan SQL pueden utilizar Hadoop\n",
    "- Hive compila las consultas redactadas en HiveQL para crear las tareas MapReduce\n",
    "\n",
    "## Ventajas\n",
    "- Es extensible: permite entre otros utilizar tipos de datos y funciones definidos por el usuario, utilizar plugins desarrollados en otros lenguajes\n",
    "- Es interoperable: Soporta diferentes formatos de datos y de ficheros\n",
    "- Ofrece altas prestaciones y tolerancia a fallos: Aprovehca las características de Hadoop\n",
    "\n",
    "## Diferencias entre Hive y BBDD relacionales\n",
    "|Hadoop con Hive| RDBMS|\n",
    "|---|---|\n",
    "|La escalabilidad es su fuerte.| Escalan peor.|\n",
    "|Diseñado para gestionar consultas de alta latencia.| Diseñados para gestionar consultas SQL de alta y baja latencia.|\n",
    "|Trabajan sobre petabytes de datos multiestructurados.| Trabajan sobre varios terabytes de datos estructurados.|\n",
    "|Eficiente en cuanto a costes.| Alto coste de escalado.|\n",
    "\n",
    "## Arquitectura de HIVE\n",
    "[03_hadoop_01]:images/03_hadoop_01\n",
    "\n",
    "- Hiveserver2:\n",
    "    - Permite conexiones remotas utilizando ODBC/JDBC de forma que se pueden utilizar aplicaciones tales como excel, toad o squirrel\n",
    "    - Estas conexiones las gestiona el componente Thrift\n",
    "    - Metastore:\n",
    "        - Contiene las definiciones de los esquemas de base de datos\n",
    "        - Es una base de datos relacional, externa a Hive\n",
    "- Clientes\n",
    "    - Terminal: beeline\n",
    "    - Interfaz gráfico web: HUE\n",
    "    - Herramientas legadas: Toad, SQuirrel ... a través de JDBC/ODBC\n",
    "- Además, otro componente es HCatalog\n",
    "    - Permite que otras herramientas (ej. Pig) puedan leery escribir esquemas de datos almacenados en el metastore\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones\n",
    "\n",
    "- Crear Base de datos:\n",
    "    - Se crea en el metastore\n",
    "    > CREATE DATABASE\n",
    "    \n",
    "- Crear tabla\n",
    "    - Se crea en el metastore\n",
    "    > CREATE EXTERNAL TABLE dataexternal( id STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LOCATION '/path/to/dataexternal';\n",
    "    \n",
    "- Selección de datos\n",
    "    - Consulta al metastore info de esa tabla para elaborar el plan de ejecución\n",
    "    - Hiveserver crea el plan de ejecución de la consulta (el trabajo MapReduce) y envía al ResManager para ejecutarlo\n",
    "    - ResManager asigna el AppMaster de este trabajo a un nodo. Además informa al AppMaster de los recursos disponibles.\n",
    "    - AppMaster:\n",
    "        - consulta al NM dónde están los ficheros de entrada\n",
    "        - solicita al ResManager contenedores en los nodemanagers que almacenan los datos\n",
    "        - contacta con los NodeManagers para crear contenedores\n",
    "    - Containers \n",
    "        - consultan al Namenode dónde están los ficheros de entrada\n",
    "        - leen datos de entrada y ejecutan los map\n",
    "    - Los map finalizan. Su salida se almacena en local y se envía a los reduce que comienzan su ejecución\n",
    "    - Los reduce finalizan. La salida se almacena en HDFS\n",
    "    - AppManager informa al ResManager de su finalización\n",
    "    - ResManager informa a Nodemanagers y a Hiveserver del final de la ejecución\n",
    "    - HiveServer \n",
    "        - consulta Namenode para recuperar la salida\n",
    "        - lee bloques de datanodes\n",
    "        - resuelve resultados al cliente\n",
    "    > SELECT COUNT(gsd19024) FROM dataexternal WHERE id = '20'\n",
    "    \n",
    "- Modelo de datos\n",
    "    - Hive soporta una variedad de tipos de datos que se dividen en dos categorías:\n",
    "        - Primitivos\n",
    "            - Numéricos, string, fecha/hora, misc.\n",
    "        - Complejos\n",
    "            - STRUCT\n",
    "            - MAP\n",
    "            - ARRAY\n",
    "\n",
    "- Tablas\n",
    "    - En Hive, las tablas son entidades ``virtuales``, ya que los datos se encuentran almacenados en ficheros en HDFS\n",
    "    - Existen dos tipos de tablas en Hive\n",
    "        - Tablas gestionadas (managed tables): también llamadas tablas internas\n",
    "            - Hive controla la tabla por completo. Sus datos están almacenados en un subdirectorio gestionado únicamente por Hive\n",
    "            - Cuando eliminiamos la tabla, se eliminan también sus datos\n",
    "        - Tablas externas\n",
    "            - Los datos están almacenados fuera del control de Hive\n",
    "            - Cuando se elimina la tabla, solamente se eliminan los metadatos, mientras que los datos persisten\n",
    "            \n",
    "- Particiones\n",
    "    - Definen la forma en que Hive almacena físicamente los datos en disco\n",
    "    - Permiten agrupar las filas basándose en el valor de alguna columna de interés\n",
    "    - Es de interés para mejorar la eficiencia de las consultas que requieren escaneos completos de tablas\n",
    "    - Una partición en Hive es una carpeta en HDFS\n",
    "    \n",
    "- Buckets\n",
    "    - Complementan a las particiones para organizar mejor los datos y de esta forma realizar de forma más eficiente las consultas\n",
    "    - Al contrario que las particiones, se define un número concreto de buckets, que no se puede modificar tras crear la tabla\n",
    "    - Los buckets están basados en que items con el mismo valor en una función hash van al mismo bucket\n",
    "    \n",
    "|Particiones| Buckets|\n",
    "|---|---|\n",
    "|Carpeta en HDFS| Fichero en HDFS|\n",
    "|Número indefinido de particiones|Número definido de buckets|\n",
    "|La columna sobre la que realizamos la partición no se incluye en el esquema de la tabla de Hive|La columna sobre la que realizamos el bucket sí se incluye en el esquema de la tabla de Hive|Número de particiones variable|Una vez creada la tabla, no se pueden modificar sus buckets|\n",
    "|Tamaños de particiones indiferentes|Idealmente, todos los buckets tienen el mismo tamaño.|\n",
    "\n",
    "- Vistas\n",
    "    - Las vistas son tablas virtuales que constituyen una ventana hacia la tabla subyacente (conocida como tabla ``base``)\n",
    "    - Al acceder a una vista, estamos accediendo a la tabla base\n",
    "    \n",
    "- Joins\n",
    "    - Un join se utiliza para recupèrar datos de interés de dos o más tablas basados en un campo común\n",
    "    - Hive solamente soporta joins de igualdad\n",
    "    - Poner a la derecha la tabla más grande para realizar un mapjoin si la tabla pequeña cabe en memoria en los mappers\n",
    "    \n",
    "- Formatos de serialización y deserialización (SERDE)\n",
    "    - Serialización y deserialización (o serde) consiste en leer y escribir datos de una variedad de formatos\n",
    "    - Algunos de los serdes que proporciona Hive por defecto son:\n",
    "        - LazySimpleSerDe: básico, se usa por defecto\n",
    "        - RegexSerDe: utiliza expresiones regulares\n",
    "        - AvroSerDe: utiliza el formato de archivo Avro\n",
    "        - OrcSerde: utiliza el formato de archivo ORC\n",
    "        - ParquetHiveSerDe: utiliza el formato de archivo Parquet\n",
    "        - JSONSerDe: utiliza el formato de archivo JSON\n",
    "        - CSVSerDe: utiliza el formato de archivo CSV\n",
    "        \n",
    "- Planes de ejecución de consultas\n",
    "    - Calcular estadísticas de las tablas, para el cálculo de los planes de ejecución\n",
    "    - Orden\n",
    "        >ANALYZE TABLE dataexternal COMPUTE STATISTICS;\n",
    "        \n",
    "- Transacciones\n",
    "    - Hive proporciona ACID a nivel de fila, de forma que una aplicación puede añadir filas mientras que otra lee de la misma partición sin interferir\n",
    "    - Limitaciones\n",
    "        - Transacciones deshabilitadas por defecto\n",
    "        - BEGIN, COMMIT Y ROLLBACK no están soportadas aún, las opeoraciones del lenguaje son auto-commit\n",
    "        - Solamente sobre tablas con buckets y en formato ORC\n",
    "    \n",
    "- Extensibilidad\n",
    "    - Hive ofrece la posibilidad de implementar nuevas funcionalidades, mediante funciones definidas por el usuario (User Defined Functions, UDFs) en Java, Python\n",
    "    \n",
    "- Compresión\n",
    "    - Hive permite comprimir datos tanto finales como intermedios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIG\n",
    "\n",
    "## ¿Qué es PIG?\n",
    "\n",
    "- Pig es una herramienta que permite programar MapReduce sin tener que \"pensar\" en MapReduce\n",
    "- Pig permite ejecutar flujos de datos en un cluster paralelo distribuido Apache Hadoop\n",
    "- Utiliza un lenguaje llamado Pig Latin para definir dichos flujos de datos\n",
    "- Pig Latin incluye operadores para muchos operadores tradicionales (reunión, ordenación, filtrado, ...)\n",
    "- Además, Pig Latin permite el desarrollo de funciones ad-hoc para lectura, procesamiento y escritura de datos\n",
    "- Pig proporciona un enfoque procedural (al contrario que Hive que es declarativo) para el proceso de datos\n",
    "    - Pig permite la definición de los detalles del procesamiento de los datos, sin tener que confiar en analizadores de consultas que generen un plan de ejcución eficiente.\n",
    "    \n",
    "## Comparando Pig Latin con MapReduce\n",
    "\n",
    "|Pig |MapReduce|\n",
    "|---|---|\n",
    "|Lenguaje de alto nivel| Paradigma de bajo nivel|\n",
    "|Minimiza la complejidad del código (1 consulta Pig puede equivaler a múltiples trabajos MapReduce) y el tiempo de desarrollo|Código aprox. 20 veces más largo y tiempo de desarrollo aprox. 16 veces mayor (es necesario programar cada trabajo individualmente)|\n",
    "|Proporciona operadores: sort, join, filtros...| Estos operadores se deben implementar manualmente|\n",
    "|Proporciona tipos de datos anidados,como tuplas o bags|Tipos de datos simples|\n",
    "\n",
    "## Comparando Pig Lating con SQL\n",
    "\n",
    "|Pig Latin| SQL|\n",
    "|---|---|\n",
    "|Describe el procedimiento a aplicar a unos datos de entrada.|Se centra en responder a una pregunta, pero no describe cómo se debe responder.|\n",
    "|Se pueden enlazar diversas operaciones en el orden en que se realizan, sin necesidad de almacenar resultados intermedios en tablas temporales.|Se centra en una única pregunta cada vez, y cuando se ecesita más de una operación, es necesario escribir subconsultas separadas o almacenar resultados en tablas intermedias.|\n",
    "\n",
    "## Estructura de PIG\n",
    "\n",
    "- Pig Latin\n",
    "    - Lenguaje de programación en el que se desarrollan los scripts para Pig\n",
    "- Grunt Shell\n",
    "    - Shell que permite la ejecución de Pig interactivas\n",
    "- Pig Server\n",
    "    - Ejecuta los scripts Pig\n",
    "- Parser\n",
    "    - Chequea la sintaxis de los scripts y da como resultado un grafo dirigido acíclico (DAG).\n",
    "    - El DAG está formado por nodos que son los operadores lógicos y por aristas que son los flujos de datos\n",
    "- Optimizer\n",
    "    - Toma el DAG generado por el parser y realisza las modificaciones necsarias para minimizar los datos que están en procesamiento en cada momento\n",
    "    - Entre otras cosas:\n",
    "        - Elimina columnas innecesarias\n",
    "        - Elimina claves map que no se utilizan\n",
    "- Compiler\n",
    "    - Genera los trabajos MapReduce para el código optimizado\n",
    "- Execution engine\n",
    "    - Se encarga de la ejecución de los trabajos MapReduce\n",
    "    - Las salidas de estos trabajos se pueden almacenar en HDFS o mostrarse por pantalla\n",
    "- Tipos de datos primitivos\n",
    "    - Entre otros: int, long, float, double, chararrary, boolean, datetime, bytearray\n",
    "    - Map\n",
    "        - Almacena una clave de tipo chararray y su valor asociado\n",
    "        - El tipo de datos del valor puede ser uno complejo\n",
    "        - Si el tipo de datos no se puede determinar, Pig utiliza bnytearray\n",
    "        - La asociación entre la clave y el valor se define utilizando el símbolo #, y las claves tienen que ser únicas\n",
    "        - Sintáxis: [key#value, key1#value1,...]\n",
    "    - Tupla\n",
    "        - Es una colección de valores de longitud fija y ordenados. Una tupla es similar a un registro en una tabla SQL pero sin restricciones en los tipos de datos de las columnas, y sin tener un esquema asociado\n",
    "        - Sintáxis: (value1, value2, value3, ...)\n",
    "    - Bag\n",
    "        - Es un contenedor de tuplas y otras bags. Están desordenadas, y puede haber tuplas o bas duplicadas dentro de una bag\n",
    "        - Sintáxis: {(tuple1, tuple2, ...)}\n",
    "        - Pueden ser internas o externas\n",
    "            - Interna(inner bag) es una bag dentro de una tupla\n",
    "            - Externa(outer bag) es una bag de tuplas\n",
    "    - Pig permite el anidamiento de estructuras de datos complejas donde se puede anidar una tupla dentro de otra tupla, una bag y un map\n",
    "    - Null: cualquier tipo de datos puede valuer Null\n",
    "        - El dato es desconocido, no existe, un error\n",
    "    - Para comprender mejor estos conceptos\n",
    "        - Una relación (una tabla de SQL) es una bag\n",
    "        - Una bag es una colección de tuplas\n",
    "        - Una tupla (similar a una fila de una base de datos) es un conjunto ordenado de campos\n",
    "        - Un campo es un dato\n",
    "- Esquemas\n",
    "    - Pig puede trabajar con esquemas, pero no impide trabajar sin ellos\n",
    "        - En el caso de que definamos un esquema, Pig lo aplicará a los datos que le pasemos, tanto para chequear que se cumple como para optimizadores\n",
    "        - En el caso de que no definamos un esquema, Pig tratará dichos datos de la mejor forma que pueda\n",
    "    - Además, los esquemas se pueden definir con la granularidad que queramos\n",
    "        - Ejemplo: un esquema puede contener tuplas, pero no es necesarios indicar la estructura de la tupla\n",
    "    - Pig trata de inferir los tipos de datos cuando no tiene información\n",
    "    - Se considera que Pig no es fuertemente tipado\n",
    "\n",
    "- Cargar esquemas\n",
    "    - La forma más sencilla de cargar un esquema es indicarlo cuando cargamos los datos\n",
    "        - Orden\n",
    "            > LOAD 'data' [USING function][AS Schema];\n",
    "        - Ejemplo\n",
    "            > relation = load'/path/to/file'; -- Carga un fichero de datos separados por tabuladores, utilizando la función de carga por defecto PigStorage\n",
    "            >\n",
    "            > relation = load'/path/to/file' using PigStorage(); -- Carga un fichero de datos separados por comas\n",
    "            >\n",
    "            > relation = load'/path/to/file' as (campo1,campo2) -- Carga un fichero de datos y especifica un esquema\n",
    "    - Esta forma es tediosa para esquemas con muchas columnas y para esquemas cambiantes\n",
    "    - Otra forma de especificar dónde se encuentra el esquema para los datos que se cargan\n",
    "    - Se puede utilizar un esquema creado previamente en Hive\n",
    "        - Ejemplo\n",
    "            - mdata = load'database.tabla' using org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "\n",
    "- Operaciones fundamentales sobre datos\n",
    "    - Los operadores de Pig se pueden agrupar en las siguientes familias\n",
    "        - Operaciones de control, conectables y estructurales\n",
    "        - Operadores de control\n",
    "            - Operadores de serialización: LOAD y STORE\n",
    "            - Directivas de Pig (DESCRIBE, ILLUSTRATE, REGISTER,...) que sriven para que Pig muestre información, registrar funciones, ...\n",
    "            \n",
    "- Almacenamiento de datos\n",
    "    - Almancenar datos\n",
    "        - Orden\n",
    "        > STORE alias INTO 'directory' [USING function];\n",
    "        - Ejemplos\n",
    "        > STORE procesados INTO 'path/to/file';\n",
    "            - Almacena los datos procesados separados por tabuladores en la carpeta indicada en HDFS utilizando la función de almacenamiento por defecto PigStorage\n",
    "        > STORE procesados INTO 'path/to/file' using PigStorage(',');\n",
    "            - Similar al anterior pero utilizando el separador ','\n",
    "        - Podemos escribir en una tabla existente en Hive\n",
    "        > STORE angio3 INTO 'angio3' USING org.apache.hive.hcatalog.pig.HCatStorer()\n",
    "    - Mostrar por pantalla\n",
    "        - Orden\n",
    "        > DUMP alias\n",
    "        - Ejemplo\n",
    "        > DUMP tablefile\n",
    "\n",
    "- Operaciones fundamentales sobre datos\n",
    "    - Operadores conectables\n",
    "        - Sirven para crear un trabajo solo map\n",
    "        - Cuando van antes o después de una operación estructural, van al map o al reducer\n",
    "        \n",
    "        - Operaciones de transformación:\n",
    "            - FOREACH, FOREACH..FLATTEN(tuple) modifican los contenidos de los registros de forma individual\n",
    "            - La cuenta de los registros de salida es la misma que la de los registros de entrada, pero se puede modificar su contenido o su esquema\n",
    "        - Operaciones de filtrado\n",
    "            - FILTER, SAMPLE, LIMIT, ASSERT aceptan o rechazan cada registro de forma individual\n",
    "            - Pueden producir el mismo número de registros de entrada o menos, pero cada registro tiene el mismo contenido y esquema que a la entrada\n",
    "        - Operadores de repartición\n",
    "            - SPLIT, UNION no cambian los registros, solamente los distribuyen en nuevas trablas o flujos de datos\n",
    "            - UNION produe tantos registros como la suma de su entrada\n",
    "            - SPLIT sin varios FIlTER simultaneos, los registros de salida que produce son la suma de lo que produce cada uno de sus filters\n",
    "            - Operaciones de desagrupado\n",
    "                - FOREACH..FLATTEN(bag) convierten registros que son bags de tupl,as en registros que contienen cada uno una combinación de las tuplas\n",
    "                - FLATTEN deja los contenidos de la bag como estaban y sustituyue el esquema de la bag por el esquema de sus contenidos. Cuando aplicamos esta función a un único campo, la cuenta de registros de salida es la misma que la cuenta de los elementos de todas las bags\n",
    "                - Múltiples FLATTEN generan un registro por cada combinación posible de elementos, que puede ser mucho más grande que la entrada\n",
    "    - Operaciones estructurales\n",
    "      - Estas operaciones requieren un map y un reduce\n",
    "      - Operaciones de agrupación (GROUP, COGROUP, CUBE, ROLLUP):\n",
    "          - Colocan los registros en contexto con respecto a los demás. No modifican los contenidos de los registros pero sí que recolocal su esquema\n",
    "              - Se les encuentra seguidos de FOREACH que aprovecha la agrupación generada\n",
    "              - GROUP y COGROUP emiten un registro por cada valor de agrupación\n",
    "      - Operaciones JOIN, CROSS\n",
    "          - Emparejan registros entre tablas    \n",
    "          - JOIN es una secuencia COGROUP/FLATTEN/FOREACH/ optimizada\n",
    "      - Operaciones de ordenación (ORDER BY, RANK)\n",
    "          - Ordenan su entrada: Cada registro en el fichero 00000 se coloca en orden y va antes que cualquier registro del fichero 00001... y así para todos los ficheros de entrada\n",
    "          - Esto requiere dos trabajos, primero un map para comprender la distribución de las claves utilizadas para la ordenación, y luego un MapReduce para realizar la ordenación\n",
    "      - Operaciones de unicidad (DISTINCT, COGROUP)\n",
    "          - Seleccionan/rechazan/eliminan duplicados\n",
    "          - Esto se consigue con combinaciones de los de arriba, y puede involucrar más de un trabajo MapReduce\n",
    "- Directivas de control\n",
    "    - Mostrar el esquema de una relación\n",
    "        > DESCRIBE relationship\n",
    "    - Mostrar un subconjunto de los datos de una relación\n",
    "        > SAMPLE relationship <sampling:number>\n",
    "        - Se suele utilizar en conjunción con limit y dump\n",
    "        > sampled = SAMPLE relationship 0.10\n",
    "        >\n",
    "        > limited = LIMIT sampled 5;\n",
    "        > \n",
    "        > DUMP limited\n",
    "    - Mostrar cómo se crea una relación\n",
    "        - Simula la ejecución de una relación para un subconjunto de los datos aleatorio\n",
    "            > ILLUSTRATE relationship\n",
    "    - Mostrar el plan de ejecución de un comando\n",
    "        > EXPLAIN relationship\n",
    "        \n",
    "- Operadores relacionales\n",
    "    - Genera transformaciones basadas en columnas de datos\n",
    "        > alias = FOREACH{gen_blk|nested_gen_blk}GENERATE expression;\n",
    "        >\n",
    "        > comienzo = FOREACH relationship GENERALTE ..field2\n",
    "    - Genera transformaciones basadas en filas de datos\n",
    "        > alias = FILTER alias BY expression;\n",
    "        >\n",
    "        > bb = filter test2 by pip23>100 and pin23<400;\n",
    "    - Agrupa datos en una o varias relaciones\n",
    "        > alias = GROUP alias {ALL|BY expression}[,alias ALL|BY expression ...][USING 'collected'][PARALLEL n];\n",
    "        >\n",
    "        > alias = COGROUP alias {ALL|BY expression}[,alias ALL|BY expression ...][USING 'collected'][PARALLEL n]\n",
    "        >\n",
    "        > bb = group test4 by pinpin;\n",
    "    - Ordenar resultados\n",
    "        > bb = order test4 by pinpin123 desc;\n",
    "    - INNER\n",
    "        > Z = JOIN X BY x1, Y BY y1\n",
    "    - OUTER\n",
    "        > Z = JOIN X BY $0 FULL, Y BY $0\n",
    "    - LIMIT\n",
    "        > Z = LIMIT X 10\n",
    "    - Paralelismo\n",
    "        > parallelism = group day by character parallel 5;\n",
    "- Funciones en Pig\n",
    "    - Matemáticas: log, exp, ...\n",
    "    - Comparación de cadenas: ==, matches ...\n",
    "    - Transformar cadenas: CONCAT, LOWER, ...\n",
    "    - Fecha y hora: CurrentTime, ToUnixTime, SecondsBetween,...\n",
    "    - Funciones de agregación sobre bags: AVG, MAX, MIN ....\n",
    "    - Funciones sobre bags: TOP, SUBSTRACT\n",
    "- UDF a través de Piggybank y Apache DataFu\n",
    "    - Son librerías que se distribuyen con Pig y se pueden utilizar registrando su jar con REGISTER\n",
    "    - Para utilizar una UDF, debemos usar su path completo, aunque podemos crear atajos mediante DEFINE\n",
    "    - Ejemplo\n",
    "        - Register /usr/lib/pig/piggiybank.jar\n",
    "        - DEFINE Reverse org.apache.pig.piggybank.evaluation.string.Reverser()\n",
    "        - b = FOREACH a GENERATE Reverse(char_field) AS reversed_char_field;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
